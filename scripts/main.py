import os
import sys
import json
from typing import Type, TypeVar
from dataclasses import dataclass, field, fields

import torch
import torchvision
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from torch.autograd import Variable
from torch import optim
import torch.nn as nn
import torch.nn.functional as F
import cv2
from PIL import Image
from tqdm import tqdm
import random
from matplotlib import pyplot as plt

# main.py ë§¨ ìœ„ì— ì¶”ê°€
import sys
sys.path.insert(0, r"C:\Users\wonseok\cocoapi\PythonAPI")

# ì´ì œ ì •ìƒ import
from pycocotools.coco import COCO
import numpy as np


sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from model.Unet import UNet

# âœ… COCO ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì¶”ê°€
class COCOClassificationDataset(Dataset):
    """
    COCO ë°ì´í„°ì…‹ì„ ë¶„ë¥˜ ì‘ì—…ìš©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤
    ê° ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ ë©´ì ì´ í° ê°ì²´ì˜ í´ë˜ìŠ¤ë¥¼ í•´ë‹¹ ì´ë¯¸ì§€ì˜ ë¼ë²¨ë¡œ ì‚¬ìš©
    """
    
    def __init__(self, root_dir, annotation_file, transform=None, max_classes=80, min_area=1000):
        """
        Args:
            root_dir: ì´ë¯¸ì§€ í´ë” ê²½ë¡œ (ì˜ˆ: 'coco/images/train2017')
            annotation_file: ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ (ì˜ˆ: 'coco/annotations/instances_train2017.json')
            transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜
            max_classes: ì‚¬ìš©í•  ìµœëŒ€ í´ë˜ìŠ¤ ìˆ˜
            min_area: ìµœì†Œ ê°ì²´ ë©´ì  (ì‘ì€ ê°ì²´ ì œì™¸)
        """
        self.root_dir = root_dir
        self.transform = transform
        self.max_classes = max_classes
        self.min_area = min_area
        
        print(f"ğŸ“‚ COCO ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        print(f"   ğŸ“ ì´ë¯¸ì§€ ê²½ë¡œ: {root_dir}")
        print(f"   ğŸ“„ ì–´ë…¸í…Œì´ì…˜: {annotation_file}")
        
        # COCO API ì´ˆê¸°í™”
        self.coco = COCO(annotation_file)
        
        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        self.categories = self.coco.loadCats(self.coco.getCatIds())
        self.category_mapping = self._create_category_mapping()
        
        # ì´ë¯¸ì§€-ë¼ë²¨ ë°ì´í„° ì¤€ë¹„
        self.image_data = []
        self._prepare_dataset()
        
        print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ:")
        print(f"   ğŸ“Š ì´ ì´ë¯¸ì§€ ìˆ˜: {len(self.image_data)}")
        print(f"   ğŸ·ï¸  í´ë˜ìŠ¤ ìˆ˜: {len(self.category_mapping)}")
    
    def _create_category_mapping(self):
        """COCO ì¹´í…Œê³ ë¦¬ IDë¥¼ ì—°ì†ëœ í´ë˜ìŠ¤ IDë¡œ ë§¤í•‘"""
        # ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë“¤ ì„ íƒ (max_classesë§Œí¼)
        category_counts = {}
        ann_ids = self.coco.getAnnIds()
        
        print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ ë¹ˆë„ ë¶„ì„ ì¤‘... (ì „ì²´ ì–´ë…¸í…Œì´ì…˜: {len(ann_ids)}ê°œ)")
        
        for ann_id in ann_ids[:10000]:  # ìƒ˜í”Œë§ìœ¼ë¡œ ë¹ ë¥´ê²Œ ê³„ì‚°
            ann = self.coco.loadAnns([ann_id])[0]
            cat_id = ann['category_id']
            category_counts[cat_id] = category_counts.get(cat_id, 0) + 1
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ í´ë˜ìŠ¤ë“¤ ì„ íƒ
        sorted_cats = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
        top_categories = [cat_id for cat_id, count in sorted_cats[:self.max_classes]]
        
        # ì—°ì†ëœ IDë¡œ ë§¤í•‘
        mapping = {cat_id: idx for idx, cat_id in enumerate(top_categories)}
        
        print(f"ğŸ“‹ ì„ íƒëœ ìƒìœ„ {len(mapping)}ê°œ ì¹´í…Œê³ ë¦¬:")
        for cat_id, class_id in mapping.items():
            cat_info = self.coco.loadCats([cat_id])[0]
            print(f"   {class_id}: {cat_info['name']} (ID: {cat_id})")
        
        return mapping
    
    def _prepare_dataset(self):
        """ì´ë¯¸ì§€-ë¼ë²¨ ìŒ ë°ì´í„° ì¤€ë¹„"""
        print(f"ğŸ”„ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
        
        # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ ì–´ë…¸í…Œì´ì…˜ë§Œ ê°€ì ¸ì˜¤ê¸°
        selected_cat_ids = list(self.category_mapping.keys())
        ann_ids = self.coco.getAnnIds(catIds=selected_cat_ids)
        
        # ì´ë¯¸ì§€ë³„ë¡œ ì–´ë…¸í…Œì´ì…˜ ê·¸ë£¹í™”
        image_annotations = {}
        
        for ann_id in tqdm(ann_ids, desc="ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬"):
            ann = self.coco.loadAnns([ann_id])[0]
            img_id = ann['image_id']
            cat_id = ann['category_id']
            area = ann['area']
            
            # ë„ˆë¬´ ì‘ì€ ê°ì²´ëŠ” ì œì™¸
            if area < self.min_area:
                continue
                
            if cat_id in self.category_mapping:
                if img_id not in image_annotations:
                    image_annotations[img_id] = []
                
                image_annotations[img_id].append({
                    'category_id': self.category_mapping[cat_id],
                    'area': area,
                    'bbox': ann['bbox']
                })
        
        # ê° ì´ë¯¸ì§€ì˜ ì£¼ìš” ê°ì²´ ê²°ì • (ê°€ì¥ í° ë©´ì )
        for img_id, annotations in tqdm(image_annotations.items(), desc="ì´ë¯¸ì§€ ë¼ë²¨ ìƒì„±"):
            if not annotations:
                continue
                
            # ì´ë¯¸ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            img_info = self.coco.loadImgs([img_id])[0]
            img_path = os.path.join(self.root_dir, img_info['file_name'])
            
            # ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if not os.path.exists(img_path):
                continue
            
            # ê°€ì¥ í° ê°ì²´ì˜ í´ë˜ìŠ¤ë¥¼ ë¼ë²¨ë¡œ ì„ íƒ
            largest_annotation = max(annotations, key=lambda x: x['area'])
            
            self.image_data.append({
                'image_path': img_path,
                'label': largest_annotation['category_id'],
                'image_id': img_id,
                'area': largest_annotation['area']
            })
    
    def __len__(self):
        return len(self.image_data)
    
    def __getitem__(self, idx):
        item = self.image_data[idx]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(item['image_path']).convert('RGB')
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {item['image_path']}")
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            image = Image.new('RGB', (224, 224), color=(128, 128, 128))
        
        # ì „ì²˜ë¦¬ ì ìš©
        if self.transform:
            image = self.transform(image)
        
        return image, item['label']
    
    def get_class_names(self):
        """í´ë˜ìŠ¤ ì´ë¦„ ëª©ë¡ ë°˜í™˜"""
        class_names = [''] * len(self.category_mapping)
        for cat_id, class_id in self.category_mapping.items():
            cat_info = self.coco.loadCats([cat_id])[0]
            class_names[class_id] = cat_info['name']
        return class_names

T = TypeVar('T')

def parse_config(config_path: str, cls: Type[T]) -> T:
    with open(config_path, "r") as f:
        config_data = json.load(f)
    
    cls_fields = {field.name for field in fields(cls)}
    filtered_data = {k: v for k, v in config_data.items() if k in cls_fields}
    
    return cls(**filtered_data)

@dataclass
class Config:
    model_name: str = field(
        default="UNet_COCO",
        metadata={"help": "The name of the model to use."}
    )

    hidden_size: int = field(
        default=768,
        metadata={"help": "The size of the hidden layer."}
    )

    n_classes: int = field(
        default=10,  # COCOì—ì„œ ì„ íƒí•  í´ë˜ìŠ¤ ìˆ˜ (ì¡°ì • ê°€ëŠ¥)
        metadata={"help": "The number of classes to classify."}
    )

    batch_size: int = field(
        default=16,  # COCO ì´ë¯¸ì§€ê°€ í¬ë¯€ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¤„ì„
        metadata={"help": "The batch size to use."}
    )

    opt_name: str = field(
        default="adam",
        metadata={"help": "The name of the optimizer to use."}
    )

    lr: float = field(
        default=1e-4,  # í° ë°ì´í„°ì…‹ì— ë§ê²Œ í•™ìŠµë¥  ì¡°ì •
        metadata={"help": "The learning rate to use."}
    )

    num_epochs: int = field(
        default=20,  # í° ë°ì´í„°ì…‹ì´ë¯€ë¡œ ì—í¬í¬ ì¤„ì„
        metadata={"help": "The number of epochs to train for."}
    )

    early_stopping: int = field(
        default=5,
        metadata={"help": "The number of epochs to wait before early stopping."}
    )

    p: float = field(
        default=0.1,
        metadata={"help": "The dropout rate to use."}
    )
    
    # âœ… COCO ê´€ë ¨ ì„¤ì • ì¶”ê°€
    coco_data_dir: str = field(
        default="./coco",
        metadata={"help": "COCO dataset directory path"}
    )
    
    image_size: int = field(
        default=224,
        metadata={"help": "Input image size"}
    )

def train(model, train_loader, criterion, optimizer, device):
    epoch_loss = 0
    correct = 0
    total = 0
    
    model.train()    
    for batch_idx, (x, y) in enumerate(tqdm(train_loader, desc="Training")):  
        x = x.to(device)
        y = y.to(device)
            
        optimizer.zero_grad()                
        y_pred, _ = model(x)  
        
        loss = criterion(y_pred, y)  
        loss.backward()        
        optimizer.step()        
        
        epoch_loss += loss.item()
        
        _, predicted = torch.max(y_pred, 1)
        total += y.size(0)
        correct += (predicted == y).sum().item()
        
    epoch_loss /= len(train_loader)  
    epoch_acc = correct / total
       
    return epoch_loss, epoch_acc

def validate(model, val_loader, criterion, device):
    epoch_loss = 0
    correct = 0
    total = 0
    
    model.eval()    
    with torch.no_grad():        
        for batch_idx, (x, y) in enumerate(tqdm(val_loader, desc="Validation")):  
            x = x.to(device)
            y = y.to(device)
            y_pred, _ = model(x) 
            loss = criterion(y_pred, y)  

            epoch_loss += loss.item()
            
            _, predicted = torch.max(y_pred, 1)
            total += y.size(0)
            correct += (predicted == y).sum().item()
        
    epoch_loss /= len(val_loader)  
    epoch_acc = correct / total
    
    return epoch_loss, epoch_acc

loss_train = []
loss_val = []

def main():
    config = Config()

    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        config = parse_config(sys.argv[1], Config)

    os.makedirs(f"{config.model_name}_logs", exist_ok=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # âœ… ëª¨ë¸ ìƒì„±
    model = UNet(config, output_dim=config.n_classes)
    model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config.lr)
    print(f"ğŸ“¦ ëª¨ë¸: {config.model_name}, í´ë˜ìŠ¤ ìˆ˜: {config.n_classes}")

    # âœ… COCOìš© ì „ì²˜ë¦¬ (ImageNet ì •ê·œí™” ì‚¬ìš©)
    transform = transforms.Compose([
        transforms.Resize((config.image_size, config.image_size)),
        transforms.RandomHorizontalFlip(p=0.5),  # ë°ì´í„° ì¦ê°•
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])  # ImageNet ì •ê·œí™”
    ])

    # ê²€ì¦ìš© ì „ì²˜ë¦¬ (ë°ì´í„° ì¦ê°• ì œì™¸)
    val_transform = transforms.Compose([
        transforms.Resize((config.image_size, config.image_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])

    # âœ… COCO ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
    train_img_dir = os.path.join(config.coco_data_dir, 'images', 'train2017')
    val_img_dir = os.path.join(config.coco_data_dir, 'images', 'val2017')
    train_ann_file = os.path.join(config.coco_data_dir, 'annotations', 'instances_train2017.json')
    val_ann_file = os.path.join(config.coco_data_dir, 'annotations', 'instances_val2017.json')

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    required_paths = [train_img_dir, val_img_dir, train_ann_file, val_ann_file]
    missing_paths = [path for path in required_paths if not os.path.exists(path)]
    
    if missing_paths:
        print("âŒ COCO ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤:")
        for path in missing_paths:
            print(f"   - {path}")
        print("\nğŸ’¡ COCO ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
        print("   1. https://cocodataset.org/#download")
        print("   2. ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¡œ ìë™ ë‹¤ìš´ë¡œë“œ")
        return

    # âœ… COCO ë°ì´í„°ì…‹ ìƒì„±
    print("ğŸ“Š COCO í›ˆë ¨ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
    train_dataset = COCOClassificationDataset(
        root_dir=train_img_dir,
        annotation_file=train_ann_file,
        transform=transform,
        max_classes=config.n_classes
    )
    
    print("\nğŸ“Š COCO ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
    val_dataset = COCOClassificationDataset(
        root_dir=val_img_dir,
        annotation_file=val_ann_file,
        transform=val_transform,
        max_classes=config.n_classes
    )

    # ë°ì´í„°ë¡œë” ìƒì„±
    trainloader = DataLoader(train_dataset, batch_size=config.batch_size,  
                            shuffle=True, num_workers=4, pin_memory=True)
    testloader = DataLoader(val_dataset, batch_size=config.batch_size,  
                           shuffle=False, num_workers=4, pin_memory=True)

    # í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
    class_names = train_dataset.get_class_names()
    print(f"\nğŸ·ï¸  í´ë˜ìŠ¤ ëª©ë¡: {class_names}")
    
    best_valid_loss = float('inf')  
    best_valid_acc = 0.0
    
    print(f"\nğŸš€ í›ˆë ¨ ì‹œì‘! (ì´ {config.num_epochs}ê°œ ì—í¬í¬)")
    print(f"   ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"   ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ")
    
    for epoch in range(config.num_epochs):
        print(f"\nğŸ“… Epoch {epoch+1}/{config.num_epochs}")
        print("-" * 60)
        
        train_loss, train_acc = train(model, trainloader, criterion, optimizer, device)
        valid_loss, valid_acc = validate(model, testloader, criterion, device)
        
        if valid_acc > best_valid_acc:  
            best_valid_acc = valid_acc
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), f'{config.model_name}_logs/best_model.pth')
            print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ë¨ (ì •í™•ë„: {best_valid_acc*100:.2f}%)")

        loss_train.append(train_loss)
        loss_val.append(valid_loss)

        print(f'ğŸ”¸ Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:6.2f}%')
        print(f'ğŸ”¹ Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc*100:6.2f}%')
        
        # Early Stopping
        if config.early_stopping and epoch > config.early_stopping:
            if valid_acc < best_valid_acc:
                print("ğŸ›‘ Early stopping ì ìš©ë¨")
                break
    
    # ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(loss_train, 'b-', label='Train Loss')
    plt.plot(loss_val, 'r-', label='Val Loss')    
    plt.title('Loss Curves')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    train_acc_list = [1 - (loss_train[i] / max(loss_train)) for i in range(len(loss_train))]
    val_acc_list = [1 - (loss_val[i] / max(loss_val)) for i in range(len(loss_val))]
    plt.plot(train_acc_list, 'b-', label='Train Acc (approx)')
    plt.plot(val_acc_list, 'r-', label='Val Acc (approx)')
    plt.title('Accuracy Curves')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(f'{config.model_name}_logs/training_curves.png')
    plt.show()
    
    print(f"\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    print(f"   ğŸ† ìµœê³  ê²€ì¦ ì •í™•ë„: {best_valid_acc*100:.2f}%")
    print(f"   ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {config.model_name}_logs/best_model.pth")

if __name__ == "__main__":
    main()